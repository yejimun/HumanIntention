{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed7b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isdir, join\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import rosbag\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import os\n",
    "import pdb\n",
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8046e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = 'short_prediction_data.pt'\n",
    "dataset = torch.load(dataset_filepath)\n",
    "\n",
    "\n",
    "xobs_train, xpred_train, yintention_train, xobs_test, xpred_test, yintention_test = \\\n",
    "dataset[\"xobs_train\"], dataset[\"xpred_train\"], dataset[\"yintention_train\"], dataset[\"xobs_test\"], \\\n",
    "dataset[\"xpred_test\"], dataset[\"yintention_test\"]\n",
    "obs_seq_len, pred_seq_len = dataset[\"obs_seq_len\"], dataset[\"pred_seq_len\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d807e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "for train_index, validation_index in kf.split(xobs_train):\n",
    "    xobs_train_k, xobs_val_k = xobs_train[train_index], xobs_train[validation_index]\n",
    "    xpred_train_k, xpred_val_k = xpred_train[train_index], xpred_train[validation_index] \n",
    "    yintention_train_k, yintention_val_k = yintention_train[train_index], yintention_train[validation_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from src.mgnn.utils import seq_to_graph\n",
    "\n",
    "class TrajectoriesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        xobs,\n",
    "        xpred,\n",
    "        yintention,\n",
    "        obs_seq_len,\n",
    "        pred_seq_len,\n",
    "        ):\n",
    "        super(TrajectoriesDataset, self).__init__()\n",
    "        self.obs_seq_len = obs_seq_len\n",
    "        self.pred_seq_len = pred_seq_len\n",
    "        self.seq_len = self.obs_seq_len + self.pred_seq_len\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_seq\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start, end = self.seq_start_end[index]\n",
    "        out = [\n",
    "            self.obs_traj[start:end, :], self.pred_traj[start:end, :],\n",
    "            self.obs_traj_rel[start:end, :], self.pred_traj_rel[start:end, :],\n",
    "            self.loss_mask_rel[start:end, :], self.loss_mask[start:end, :],\n",
    "            self.v_obs[index], self.A_obs[index],\n",
    "            self.v_pred[index], self.A_pred[index],\n",
    "            self.attn_mask_obs[index], self.attn_mask_pred[index],\n",
    "        ]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "pkg_path = '..'\n",
    "sys.path.append(pkg_path)\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from src_v2.utils import average_offset_error, padding_mask\n",
    "import pdb\n",
    "\n",
    "\n",
    "class IntentionLstm(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size=64, hidden_size=64, num_layers=1, \\\n",
    "                 dropout=0., device='cuda:0'):\n",
    "        \"\"\"num_lstms indicates the number of LSTMs stacked together.\"\"\"\n",
    "        super(IntentionLstm, self).__init__()\n",
    "        self.embedding_size, self.hidden_size, self.num_layers = \\\n",
    "            embedding_size, hidden_size, num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "                    input_size=embedding_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=num_layers,\n",
    "                    batch_first=True,\n",
    "                    dropout=dropout,\n",
    "                    bidirectional=False,\n",
    "                    ).to(device)\n",
    "\n",
    "        self.spatial_embedding = nn.Linear(4, embedding_size)\n",
    "        self.hidden2pos = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size, device='cuda:0'):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "        )\n",
    "    \n",
    "    def get_hf(self, so, sp, si, mode='teacher_forcing', device='cuda:0'):\n",
    "\n",
    "        \"\"\"\n",
    "        Let's first pretend there is no corner case and everything is fine.\n",
    "        inputs:\n",
    "            - so # observation sample. # tensor. size: (N, obs_seq_len, 2) # already .to(device)\n",
    "            - sp # ground truth prediction sample. # tensor. size: (N, pred_seq_len, 2) # already .to(device)\n",
    "            - si # intention sample. # tensor. size: (N, 1, 2) # already .to(device)\n",
    "            - mode # 'teacher_forcing' or 'no_forcing'.\n",
    "\n",
    "        outputs:\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_size = so.size(0) # batch_size = \n",
    "        sio = torch.ones_like(so).to(device) * si # attach to so\n",
    "        so_ebd = torch.cat((so, sio), dim=2) # (1, obs_seq_len, 4)\n",
    "        so_ebd = self.spatial_embedding(so_ebd.reshape(-1, 4))\n",
    "\n",
    "        so_ebd = so_ebd.reshape(batch_size, -1, self.embedding_size)\n",
    "        hc_0 = self.init_hidden(batch_size, device=device)\n",
    "        out, hc_t = self.lstm(so_ebd, hc_0) #  out: (batch_size, obs_seq_len, hidden_size)\n",
    "        hf = out[:, -1:, :] # (batch_size, 1, hidden_size)\n",
    "    \n",
    "        return hf\n",
    "    \n",
    "    def forward(self, so, so_lens, sp, sp_lens, si, mode='teacher_forcing', device='cuda:0'):\n",
    "        \"\"\"\n",
    "        Parallel computation.\n",
    "        inputs:\n",
    "            - so # observation sample. # tensor. size: (N, T_obs_max, 2)\n",
    "            - so_lens # T_obs for all samples in sp. # tensor. size: (N,)\n",
    "            - sp # ground truth prediction sample. # tensor. size: (N, T_pred_max, 2)\n",
    "            - sp_lens # T_pred for all samples in sp. # tensor. size: (N,)\n",
    "            - si # intention sample. # tensor. size: (N, 1, 2)\n",
    "            - mode # 'teacher_forcing' or 'no_forcing'.\n",
    "\n",
    "        outputs:\n",
    "            - loss # average offset error across samples.\n",
    "            - sp_pred # prediction on sp. # tensor. size: (N, T_pred_max, 2)\n",
    "        \"\"\"\n",
    "        so_intent = torch.cat((so, si * torch.ones_like(so)), dim=2).float().to(device) # N, T_obs_max, 4\n",
    "        sp_intent = torch.cat((sp, si * torch.ones_like(sp)), dim=2).float().to(device) # N, T_pred_max, 4\n",
    "        si = si.float().to(device)\n",
    "        sp = sp.float().to(device)\n",
    "        sp_mask = padding_mask(sp_lens).float().to(device) # (N, T_pred_max, 1)\n",
    "\n",
    "        batch_size, _, input_channel = so_intent.shape\n",
    "        T_pred_max = sp.shape[1]\n",
    "\n",
    "        so_intent_ebd = self.spatial_embedding(so_intent.reshape(-1, input_channel))\n",
    "        so_intent_ebd = so_intent_ebd.reshape(batch_size, -1, self.embedding_size) # (N, T_obs_max, embed_size)\n",
    "\n",
    "        packed_so_intent_ebd = torch.nn.utils.rnn.pack_padded_sequence(so_intent_ebd, so_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        out, hc_t = self.lstm(packed_so_intent_ebd)\n",
    "\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "\n",
    "        so_last_idx = (torch.ones(batch_size, 1, self.hidden_size)*(so_lens-1).reshape(-1,1,1)).long().to(device) # so_lens-1 -> len-1 is idx\n",
    "        out = out.gather(1, so_last_idx) # dim=1 -> time dimension # (batch_size, 1, hidden_size)\n",
    "\n",
    "        pred_list = []\n",
    "\n",
    "        pred_ts = self.hidden2pos(out.reshape(batch_size, -1)).reshape(batch_size, 1, 2) # (batch, 1, 2)\n",
    "\n",
    "        pred_list.append(pred_ts)\n",
    "        pred_ts_past = pred_ts\n",
    "        for ts in range(1, T_pred_max):\n",
    "            if mode == 'teacher_forcing':\n",
    "                pred_ts_past_intent = sp_intent[:, ts-1:ts] # torch.cat((pred_ts_past, si), dim=2) # (batch, 1, 4) \n",
    "            elif mode == 'no_forcing':\n",
    "                pred_ts_past_intent = torch.cat((pred_ts_past, si), dim=2) # (batch, 1, 4)\n",
    "            pred_ts_past_intent_ebd = self.spatial_embedding(pred_ts_past_intent.reshape(-1, 4))\n",
    "            pred_ts_past_intent_ebd = pred_ts_past_intent_ebd.reshape(batch_size, 1, self.embedding_size)\n",
    "            out, hc_t = self.lstm(pred_ts_past_intent_ebd, hc_t) #(N, 1, embed_size)\n",
    "            pred_ts = self.hidden2pos(out.reshape(batch_size, -1)).reshape(batch_size, 1, 2)\n",
    "            pred_list.append(pred_ts)\n",
    "            pred_ts_past = pred_ts\n",
    "\n",
    "        sp_pred = torch.cat(pred_list, dim=1) # (N, T_pred_max, 2)\n",
    "        loss = average_offset_error(sp, sp_pred, sp_mask)\n",
    "        return loss, sp_pred\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward_origin(self, so, sp, si, mode='teacher_forcing', device='cuda:0'):\n",
    "\n",
    "        \"\"\"\n",
    "        Let's first pretend there is no corner case and everything is fine.\n",
    "        inputs:\n",
    "            - so # observation sample. # tensor. size: (N, obs_seq_len, 2) # already .to(device)\n",
    "            - sp # ground truth prediction sample. # tensor. size: (N, pred_seq_len, 2) # already .to(device)\n",
    "            - si # intention sample. # tensor. size: (N, 1, 2) # already .to(device)\n",
    "            - mode # 'teacher_forcing' or 'no_forcing'.\n",
    "\n",
    "        outputs:\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_size = so.size(0) # batch_size = \n",
    "        sio = torch.ones_like(so).to(device) * si # attach to so\n",
    "        so_ebd = torch.cat((so, sio), dim=2) # (1, obs_seq_len, 4)\n",
    "        so_ebd = self.spatial_embedding(so_ebd.reshape(-1, 4))\n",
    "\n",
    "        so_ebd = so_ebd.reshape(batch_size, -1, self.embedding_size)\n",
    "        hc_0 = self.init_hidden(batch_size, device=device)\n",
    "        out, hc_t = self.lstm(so_ebd, hc_0) #  out: (batch_size, obs_seq_len, hidden_size)\n",
    "        hf = out[:, -1:, :] # (batch_size, 1, hidden_size)\n",
    "\n",
    "        sp_pred = torch.zeros_like(sp)\n",
    "        sp_pred_ts = self.hidden2pos(hf.reshape(batch_size, -1)).reshape(batch_size, 1, 2)\n",
    "        sp_pred[:, 0:1, :] = sp_pred_ts\n",
    "        sp_pred_ts_prev = sp_pred_ts\n",
    "\n",
    "        pred_seq_len = sp.size(1)\n",
    "\n",
    "        for ts in range(1, pred_seq_len):\n",
    "            if mode == 'teacher_forcing':\n",
    "                sp_ts_prev = sp[:, ts-1:ts] # (batch_size, 1, 2)\n",
    "                sp_pred_ts_prev_ebd = torch.cat((sp_ts_prev, si), dim=2) # (1, 1, 4)\n",
    "                sp_pred_ts_prev_ebd = self.spatial_embedding(sp_pred_ts_prev_ebd .reshape(-1, 4)) # embedding from ground truth data\n",
    "            elif mode == 'no_forcing':\n",
    "                sp_pred_ts_prev_ebd = torch.cat((sp_pred_ts_prev, si), dim=2) # (1, 1, 4)\n",
    "                sp_pred_ts_prev_ebd = self.spatial_embedding(sp_pred_ts_prev_ebd.reshape(-1, 4))\n",
    "            else:\n",
    "                print('Error on mode.')\n",
    "                sys.exit(1)\n",
    "\n",
    "            sp_pred_ts_prev_ebd = sp_pred_ts_prev_ebd.reshape(batch_size, 1, self.embedding_size)\n",
    "\n",
    "            out, hc_t = self.lstm(sp_pred_ts_prev_ebd, hc_t)\n",
    "            hf = out[:, -1:, :]\n",
    "            \n",
    "            sp_pred_ts = self.hidden2pos(hf.reshape(batch_size, -1)).reshape(batch_size, 1, 2)\n",
    "            sp_pred[:, ts:ts+1, :] = sp_pred_ts\n",
    "            sp_pred_ts_prev = sp_pred_ts\n",
    "\n",
    "#         loss = torch.mean(((sp_pred-sp) ** 2).sum(2))\n",
    "        loss = torch.mean((((sp_pred-sp) ** 2).sum(2))**(1./2))\n",
    "\n",
    "        return loss, sp_pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

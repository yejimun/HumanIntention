{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed7b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isdir, join\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import rosbag\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import os\n",
    "import pdb\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8046e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = 'short_prediction_data.pt'\n",
    "dataset = torch.load(dataset_filepath)\n",
    "\n",
    "xobs_train, xpred_train, yintention_train, xobs_test, xpred_test, yintention_test = \\\n",
    "dataset[\"xobs_train\"], dataset[\"xpred_train\"], dataset[\"yintention_train\"], dataset[\"xobs_test\"], \\\n",
    "dataset[\"xpred_test\"], dataset[\"yintention_test\"]\n",
    "obs_seq_len, pred_seq_len = dataset[\"obs_seq_len\"], dataset[\"pred_seq_len\"]\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5395b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoriesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        xobs,\n",
    "        xpred,\n",
    "        yintention,\n",
    "        obs_seq_len=4,\n",
    "        pred_seq_len=6,\n",
    "        ):\n",
    "        super(TrajectoriesDataset, self).__init__()\n",
    "        assert xobs.shape[0]==xpred.shape[0]==yintention.shape[0]\n",
    "        assert xobs.shape[1]==obs_seq_len and xpred.shape[1]==pred_seq_len\n",
    "        self.obs_seq_len = obs_seq_len\n",
    "        self.pred_seq_len = pred_seq_len\n",
    "        self.seq_len = self.obs_seq_len + self.pred_seq_len\n",
    "        self.xobs = xobs\n",
    "        self.xpred = xpred\n",
    "        self.yintention = yintention\n",
    "        self.num_seq = self.xobs.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_seq\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start, end = self.seq_start_end[index]\n",
    "        out = [\n",
    "            self.xobs[index],\n",
    "            self.xpred[index],\n",
    "            self.yintention[index],\n",
    "        ]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fad41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "for train_index, validation_index in kf.split(xobs_train):\n",
    "    xobs_train_k, xobs_val_k = xobs_train[train_index], xobs_train[validation_index]\n",
    "    xpred_train_k, xpred_val_k = xpred_train[train_index], xpred_train[validation_index] \n",
    "    yintention_train_k, yintention_val_k = yintention_train[train_index], yintention_train[validation_index]\n",
    "    dataset_train = TrajectoriesDataset(\n",
    "        xobs_train_k,\n",
    "        xpred_train_k,\n",
    "        yintention_train_k,\n",
    "        obs_seq_len=obs_seq_len,\n",
    "        pred_seq_len=pred_seq_len,\n",
    "    )\n",
    "    dataset_val = TrajectoriesDataset(\n",
    "        xobs_val_k,\n",
    "        xpred_val_k,\n",
    "        yintention_val_k,\n",
    "        obs_seq_len=obs_seq_len,\n",
    "        pred_seq_len=pred_seq_len,\n",
    "    )\n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,  \n",
    "    )\n",
    "    loader_val = DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,  \n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b963e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# pkg_path = '..'\n",
    "# sys.path.append(pkg_path)\n",
    "# import pickle\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from src_v2.utils import average_offset_error, padding_mask\n",
    "# import pdb\n",
    "\n",
    "\n",
    "class IntentionLstm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size=32,\n",
    "        hidden_size=32,\n",
    "        num_layers=1,\n",
    "        dropout=0.,\n",
    "    ):\n",
    "        super(IntentionLstm, self).__init__()\n",
    "#         self.embedding_size, self.hidden_size, self.num_layers = \\\n",
    "#             embedding_size, hidden_size, num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=2*embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=False,\n",
    "            )\n",
    "        self.spatial_embedding = nn.Linear(2, embedding_size)\n",
    "        self.intention_embedding = nn.Linear(1, embedding_size)\n",
    "        self.hidden2pos = nn.Linear(hidden_size, 2)\n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "    def forward(self, b_obs, b_intention, device=\"cuda:0\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward function.\n",
    "        inputs:\n",
    "            - b_obs: batch of observation. (batch, obs_seq_len, 2)\n",
    "            - b_intention: batch of intention. (batch, 1)\n",
    "        outputs:\n",
    "            - b_out: (batch, pred_seq_len, 2)\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_size, obs_seq_len, _ = b_obs.shape\n",
    "        b_obs = self.spatial_embedding(b_obs) # (batch, obs_seq_len, embedding_size)\n",
    "        b_intention = self.intention_embedding(b_intention.unsqueeze(-1)) # (batch, 1, embedding_size)\n",
    "        b_intention_obs = b_intention*torch.ones(batch_size, obs_seq_len, self.embedding_size).to(device)\n",
    "        \n",
    "        # PAUSE\n",
    "        \n",
    "        b_obs_intention =  torch.cat((so, si * torch.ones_like(so).to(device)), dim=2).float()\n",
    "        \n",
    "        \n",
    "        so_intent = torch.cat((so, si * torch.ones_like(so)), dim=2).float().to(device) # N, T_obs_max, 4\n",
    "        sp_intent = torch.cat((sp, si * torch.ones_like(sp)), dim=2).float().to(device) # N, T_pred_max, 4\n",
    "        si = si.float().to(device)\n",
    "        sp = sp.float().to(device)\n",
    "        sp_mask = padding_mask(sp_lens).float().to(device) # (N, T_pred_max, 1)\n",
    "\n",
    "        batch_size, _, input_channel = so_intent.shape\n",
    "        T_pred_max = sp.shape[1]\n",
    "\n",
    "        so_intent_ebd = self.spatial_embedding(so_intent.reshape(-1, input_channel))\n",
    "        so_intent_ebd = so_intent_ebd.reshape(batch_size, -1, self.embedding_size) # (N, T_obs_max, embed_size)\n",
    "\n",
    "        packed_so_intent_ebd = torch.nn.utils.rnn.pack_padded_sequence(so_intent_ebd, so_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        out, hc_t = self.lstm(packed_so_intent_ebd)\n",
    "\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "\n",
    "        so_last_idx = (torch.ones(batch_size, 1, self.hidden_size)*(so_lens-1).reshape(-1,1,1)).long().to(device) # so_lens-1 -> len-1 is idx\n",
    "        out = out.gather(1, so_last_idx) # dim=1 -> time dimension # (batch_size, 1, hidden_size)\n",
    "\n",
    "        pred_list = []\n",
    "\n",
    "        pred_ts = self.hidden2pos(out.reshape(batch_size, -1)).reshape(batch_size, 1, 2) # (batch, 1, 2)\n",
    "\n",
    "        pred_list.append(pred_ts)\n",
    "        pred_ts_past = pred_ts\n",
    "        for ts in range(1, T_pred_max):\n",
    "            if mode == 'teacher_forcing':\n",
    "                pred_ts_past_intent = sp_intent[:, ts-1:ts] # torch.cat((pred_ts_past, si), dim=2) # (batch, 1, 4) \n",
    "            elif mode == 'no_forcing':\n",
    "                pred_ts_past_intent = torch.cat((pred_ts_past, si), dim=2) # (batch, 1, 4)\n",
    "            pred_ts_past_intent_ebd = self.spatial_embedding(pred_ts_past_intent.reshape(-1, 4))\n",
    "            pred_ts_past_intent_ebd = pred_ts_past_intent_ebd.reshape(batch_size, 1, self.embedding_size)\n",
    "            out, hc_t = self.lstm(pred_ts_past_intent_ebd, hc_t) #(N, 1, embed_size)\n",
    "            pred_ts = self.hidden2pos(out.reshape(batch_size, -1)).reshape(batch_size, 1, 2)\n",
    "            pred_list.append(pred_ts)\n",
    "            pred_ts_past = pred_ts\n",
    "\n",
    "        sp_pred = torch.cat(pred_list, dim=1) # (N, T_pred_max, 2)\n",
    "        loss = average_offset_error(sp, sp_pred, sp_mask)\n",
    "        return loss, sp_pred\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward_origin(self, so, sp, si, mode='teacher_forcing', device='cuda:0'):\n",
    "\n",
    "        \"\"\"\n",
    "        Let's first pretend there is no corner case and everything is fine.\n",
    "        inputs:\n",
    "            - so # observation sample. # tensor. size: (N, obs_seq_len, 2) # already .to(device)\n",
    "            - sp # ground truth prediction sample. # tensor. size: (N, pred_seq_len, 2) # already .to(device)\n",
    "            - si # intention sample. # tensor. size: (N, 1, 2) # already .to(device)\n",
    "            - mode # 'teacher_forcing' or 'no_forcing'.\n",
    "\n",
    "        outputs:\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_size = so.size(0) # batch_size = \n",
    "        sio = torch.ones_like(so).to(device) * si # attach to so\n",
    "        so_ebd = torch.cat((so, sio), dim=2) # (1, obs_seq_len, 4)\n",
    "        so_ebd = self.spatial_embedding(so_ebd.reshape(-1, 4))\n",
    "\n",
    "        so_ebd = so_ebd.reshape(batch_size, -1, self.embedding_size)\n",
    "        hc_0 = self.init_hidden(batch_size, device=device)\n",
    "        out, hc_t = self.lstm(so_ebd, hc_0) #  out: (batch_size, obs_seq_len, hidden_size)\n",
    "        hf = out[:, -1:, :] # (batch_size, 1, hidden_size)\n",
    "\n",
    "        sp_pred = torch.zeros_like(sp)\n",
    "        sp_pred_ts = self.hidden2pos(hf.reshape(batch_size, -1)).reshape(batch_size, 1, 2)\n",
    "        sp_pred[:, 0:1, :] = sp_pred_ts\n",
    "        sp_pred_ts_prev = sp_pred_ts\n",
    "\n",
    "        pred_seq_len = sp.size(1)\n",
    "\n",
    "        for ts in range(1, pred_seq_len):\n",
    "            if mode == 'teacher_forcing':\n",
    "                sp_ts_prev = sp[:, ts-1:ts] # (batch_size, 1, 2)\n",
    "                sp_pred_ts_prev_ebd = torch.cat((sp_ts_prev, si), dim=2) # (1, 1, 4)\n",
    "                sp_pred_ts_prev_ebd = self.spatial_embedding(sp_pred_ts_prev_ebd .reshape(-1, 4)) # embedding from ground truth data\n",
    "            elif mode == 'no_forcing':\n",
    "                sp_pred_ts_prev_ebd = torch.cat((sp_pred_ts_prev, si), dim=2) # (1, 1, 4)\n",
    "                sp_pred_ts_prev_ebd = self.spatial_embedding(sp_pred_ts_prev_ebd.reshape(-1, 4))\n",
    "            else:\n",
    "                print('Error on mode.')\n",
    "                sys.exit(1)\n",
    "\n",
    "            sp_pred_ts_prev_ebd = sp_pred_ts_prev_ebd.reshape(batch_size, 1, self.embedding_size)\n",
    "\n",
    "            out, hc_t = self.lstm(sp_pred_ts_prev_ebd, hc_t)\n",
    "            hf = out[:, -1:, :]\n",
    "            \n",
    "            sp_pred_ts = self.hidden2pos(hf.reshape(batch_size, -1)).reshape(batch_size, 1, 2)\n",
    "            sp_pred[:, ts:ts+1, :] = sp_pred_ts\n",
    "            sp_pred_ts_prev = sp_pred_ts\n",
    "\n",
    "#         loss = torch.mean(((sp_pred-sp) ** 2).sum(2))\n",
    "        loss = torch.mean((((sp_pred-sp) ** 2).sum(2))**(1./2))\n",
    "\n",
    "        return loss, sp_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b63181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, loader_train, loader_val, device=\"cuda:0\"):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "  train(args, train_data_loaders, writer, logdir, device=device)\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def train(args, data_loaders, writer, logdir, device='cuda:0'):\n",
    "    print('-'*50)\n",
    "    print('Training Phase')\n",
    "    print('-'*50, '\\n')\n",
    "    loader_train, loader_val = data_loaders\n",
    "    model = st_model(args, device=device).to(device)\n",
    "    temperature_scheduler = Temp_Scheduler(args.num_epochs, args.init_temp, args.init_temp, temp_min=0.03)      \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    lr_scheduler = StepLR(optimizer, step_size=50, gamma=0.3)\n",
    "    print('Model is initialized.')\n",
    "    print('learning rate: ', args.lr)\n",
    "    checkpoint_dir = join(logdir, 'checkpoint')\n",
    "    if not isdir(checkpoint_dir):\n",
    "        makedirs(checkpoint_dir)\n",
    "    with open(join(checkpoint_dir, 'args.pickle'), 'wb') as f:\n",
    "        pickle.dump(args, f)\n",
    "    print('EPOCHS: ', args.num_epochs)\n",
    "    print('Training started.\\n')\n",
    "    train_loss_task, train_aoe_task, train_foe_task = [], [], []\n",
    "    val_loss_task, val_aoe_task, val_foe_task = [], [], []\n",
    "    for epoch in range(1, args.num_epochs+1):\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "        tau = temperature_scheduler.step()\n",
    "        train_loss_epoch, train_aoe_epoch, train_foe_epoch, train_loss_mask_epoch = [], [], [], []\n",
    "        for batch_idx, batch in enumerate(loader_train):\n",
    "            obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_rel_gt, loss_mask_rel, loss_mask, \\\n",
    "            v_obs, A_obs, v_pred_gt, A_pred_gt, attn_mask_obs, attn_mask_pred_gt = batch\n",
    "            if args.rotation_pattern is not None:\n",
    "                (v_obs, A_obs, v_pred_gt, A_pred_gt), _ = \\\n",
    "                    random_rotate_graph(args, v_obs, A_obs, v_pred_gt, A_pred_gt)\n",
    "            v_obs, A_obs, v_pred_gt, attn_mask_obs, loss_mask_rel = \\\n",
    "                v_obs.to(device), A_obs.to(device), v_pred_gt.to(device), \\\n",
    "                attn_mask_obs.to(device), loss_mask_rel.to(device)\n",
    "            if args.deterministic:\n",
    "                sampling = False\n",
    "            else:\n",
    "                sampling = True\n",
    "            results = model(v_obs, A_obs, attn_mask_obs, loss_mask_rel, tau=tau, hard=False, sampling=sampling, device=device)\n",
    "            gaussian_params_pred, x_sample_pred, info = results\n",
    "            loss_mask_per_pedestrian = info['loss_mask_per_pedestrian']\n",
    "            loss_mask_rel_full_partial = info['loss_mask_rel_full_partial']\n",
    "            if args.deterministic:\n",
    "                loss_mask_rel_pred = loss_mask_rel[:,:,-args.pred_seq_len:]\n",
    "                offset_error_sq, eventual_loss_mask = offset_error_square_full_partial(x_sample_pred, v_pred_gt, loss_mask_rel_full_partial, loss_mask_rel_pred)\n",
    "                loss = offset_error_sq.sum()/eventual_loss_mask.sum()\n",
    "            else:\n",
    "                loss = negative_log_likelihood(gaussian_params_pred, v_pred_gt, loss_mask=loss_mask_per_pedestrian)  \n",
    "            train_loss_epoch.append(loss.detach().to('cpu').item())\n",
    "            loss = loss / args.batch_size\n",
    "            loss.backward()\n",
    "            aoe = average_offset_error(x_sample_pred, v_pred_gt, loss_mask=loss_mask_per_pedestrian)\n",
    "            foe = final_offset_error(x_sample_pred, v_pred_gt, loss_mask=loss_mask_per_pedestrian)\n",
    "            train_aoe_epoch.append(aoe.detach().to('cpu').numpy())\n",
    "            train_foe_epoch.append(foe.detach().to('cpu').numpy())\n",
    "            train_loss_mask_epoch.append(loss_mask_per_pedestrian[0].detach().to('cpu').numpy())\n",
    "\n",
    "            if (batch_idx+1) % args.batch_size == 0 or batch_idx+1 == len(loader_train):\n",
    "                if args.clip_grad is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(), args.clip_grad)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        train_aoe_epoch, train_foe_epoch, train_loss_mask_epoch = \\\n",
    "            np.concatenate(train_aoe_epoch, axis=0), \\\n",
    "            np.concatenate(train_foe_epoch, axis=0), \\\n",
    "            np.concatenate(train_loss_mask_epoch, axis=0)\n",
    "        train_loss_epoch, train_aoe_epoch, train_foe_epoch = \\\n",
    "            np.mean(train_loss_epoch), \\\n",
    "            train_aoe_epoch.sum()/train_loss_mask_epoch.sum(), \\\n",
    "            train_foe_epoch.sum()/train_loss_mask_epoch.sum()\n",
    "        train_loss_task.append(train_loss_epoch)\n",
    "        train_aoe_task.append(train_aoe_epoch)\n",
    "        train_foe_task.append(train_foe_epoch)\n",
    "        training_epoch_period = time.time() - epoch_start_time\n",
    "        training_epoch_period_per_sample = training_epoch_period/len(loader_train)\n",
    "\n",
    "        val_loss_epoch, val_aoe_epoch, val_foe_epoch = inference(loader_val, model, args, mode='val', tau=tau, device=device)\n",
    "        \n",
    "        print('Epoch: {0} | train loss: {1:.4f} | val loss: {2:.4f} | train aoe: {3:.4f} | val aoe: {4:.4f} | train foe: {5:.4f} | val foe: {6:.4f} | period: {7:.2f} sec | time per sample: {8:.4f} sec'\\\n",
    "                        .format(epoch, train_loss_epoch, val_loss_epoch,\\\n",
    "                        train_aoe_epoch, val_aoe_epoch,\\\n",
    "                        train_foe_epoch, val_foe_epoch,\\\n",
    "                        training_epoch_period, training_epoch_period_per_sample))\n",
    "        if epoch % 10 == 0:\n",
    "            model_filename = join(checkpoint_dir, 'epoch_'+str(epoch)+'.pt')\n",
    "            torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': train_loss_epoch,\n",
    "                    'val_loss': val_loss_epoch,\n",
    "                    'train_aoe': train_aoe_epoch,\n",
    "                    'val_aoe': val_aoe_epoch, \n",
    "                    'train_foe': train_foe_epoch,\n",
    "                    'val_foe': val_foe_epoch,   \n",
    "                    }, model_filename)\n",
    "            print('epoch_'+str(epoch)+'.pt is saved.')\n",
    "        \n",
    "        val_loss_task.append(val_loss_epoch)\n",
    "        val_aoe_task.append(val_aoe_epoch)\n",
    "        val_foe_task.append(val_foe_epoch)\n",
    "        writer.add_scalars('loss', {'train': train_loss_task[-1], 'val': val_loss_task[-1]}, epoch)\n",
    "        writer.add_scalars('aoe', {'train': train_aoe_task[-1], 'val': val_aoe_task[-1]}, epoch)\n",
    "        writer.add_scalars('foe', {'train': train_foe_task[-1], 'val': val_foe_task[-1]}, epoch)\n",
    "\n",
    "    hist = {}\n",
    "    hist['train_loss'], hist['val_loss'] = train_loss_task, val_loss_task\n",
    "    hist['train_aoe'], hist['val_aoe'] = train_aoe_task, val_aoe_task\n",
    "    hist['train_foe'], hist['val_foe'] = train_foe_task, val_foe_task\n",
    "    with open(join(checkpoint_dir, 'train_hist.pickle'), 'wb') as f:\n",
    "        pickle.dump(hist, f)\n",
    "        print(join(checkpoint_dir, 'train_hist.pickle')+' is saved.')\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d807e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    import pathhack\n",
    "import pickle\n",
    "import time\n",
    "from os.path import join, isdir\n",
    "from os import makedirs\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from src.mgnn.utils import arg_parse, average_offset_error, final_offset_error, \\\n",
    "    negative_log_likelihood, random_rotate_graph, args2writername\n",
    "from src.gumbel_social_transformer.temperature_scheduler import Temp_Scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_batch_dataset(args, pkg_path, subfolder='train', num_workers=4, shuffle=None):\n",
    "    result_filename = args.dataset+'_dset_'+subfolder+'_batch_trajectories.pt'\n",
    "    if args.dataset == 'sdd':\n",
    "        dataset_folderpath = join(pkg_path, 'datasets/sdd/social_pool_data')\n",
    "    else:\n",
    "        dataset_folderpath = join(pkg_path, 'datasets/eth_ucy', args.dataset)\n",
    "    dset = torch.load(join(dataset_folderpath, result_filename))\n",
    "    if shuffle is None:\n",
    "        if subfolder == 'train':\n",
    "            shuffle = True\n",
    "        else:\n",
    "            shuffle = False\n",
    "    dloader = DataLoader(\n",
    "        dset,\n",
    "        batch_size=1,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers)\n",
    "    return dloader\n",
    "\n",
    "def main(args):\n",
    "    print('\\n\\n')\n",
    "    print('-'*50)\n",
    "    print('arguments: ', args)\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    np.random.seed(args.random_seed)\n",
    "    if args.batch_size != 1:\n",
    "        raise RuntimeError(\"Batch size must be 1 for BatchTrajectoriesDataset.\")\n",
    "    if args.dataset == 'sdd' and args.rotation_pattern is not None:\n",
    "        raise RuntimeError(\"SDD should not allow rotation since it uses pixels.\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('device: ', device)\n",
    "    loader_train = load_batch_dataset(args, pathhack.pkg_path, subfolder='train')\n",
    "    if args.dataset == 'sdd':\n",
    "        loader_val = load_batch_dataset(args, pathhack.pkg_path, subfolder='test') # no val for sdd\n",
    "    else:\n",
    "        loader_val = load_batch_dataset(args, pathhack.pkg_path, subfolder='val')\n",
    "        \n",
    "        \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7762401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([639, 4, 2])\n",
      "torch.Size([639, 6, 2])\n",
      "torch.Size([639, 1])\n"
     ]
    }
   ],
   "source": [
    "print(xobs_train_k.shape)\n",
    "print(xpred_train_k.shape)\n",
    "print(yintention_train_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4606aded",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.mgnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1b836d4b7f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseq_to_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTrajectoriesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.mgnn'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "pkg_path = '..'\n",
    "sys.path.append(pkg_path)\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from src_v2.utils import average_offset_error, padding_mask\n",
    "import pdb\n",
    "\n",
    "\n",
    "class IntentionLstm(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size=64, hidden_size=64, num_layers=1, \\\n",
    "                 dropout=0., device='cuda:0'):\n",
    "        \"\"\"num_lstms indicates the number of LSTMs stacked together.\"\"\"\n",
    "        super(IntentionLstm, self).__init__()\n",
    "        self.embedding_size, self.hidden_size, self.num_layers = \\\n",
    "            embedding_size, hidden_size, num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "                    input_size=embedding_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=num_layers,\n",
    "                    batch_first=True,\n",
    "                    dropout=dropout,\n",
    "                    bidirectional=False,\n",
    "                    ).to(device)\n",
    "\n",
    "        self.spatial_embedding = nn.Linear(4, embedding_size)\n",
    "        self.hidden2pos = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size, device='cuda:0'):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "        )\n",
    "    \n",
    "    def get_hf(self, so, sp, si, mode='teacher_forcing', device='cuda:0'):\n",
    "\n",
    "        \"\"\"\n",
    "        Let's first pretend there is no corner case and everything is fine.\n",
    "        inputs:\n",
    "            - so # observation sample. # tensor. size: (N, obs_seq_len, 2) # already .to(device)\n",
    "            - sp # ground truth prediction sample. # tensor. size: (N, pred_seq_len, 2) # already .to(device)\n",
    "            - si # intention sample. # tensor. size: (N, 1, 2) # already .to(device)\n",
    "            - mode # 'teacher_forcing' or 'no_forcing'.\n",
    "\n",
    "        outputs:\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_size = so.size(0) # batch_size = \n",
    "        sio = torch.ones_like(so).to(device) * si # attach to so\n",
    "        so_ebd = torch.cat((so, sio), dim=2) # (1, obs_seq_len, 4)\n",
    "        so_ebd = self.spatial_embedding(so_ebd.reshape(-1, 4))\n",
    "\n",
    "        so_ebd = so_ebd.reshape(batch_size, -1, self.embedding_size)\n",
    "        hc_0 = self.init_hidden(batch_size, device=device)\n",
    "        out, hc_t = self.lstm(so_ebd, hc_0) #  out: (batch_size, obs_seq_len, hidden_size)\n",
    "        hf = out[:, -1:, :] # (batch_size, 1, hidden_size)\n",
    "    \n",
    "        return hf\n",
    "    \n",
    "    def forward(self, so, so_lens, sp, sp_lens, si, mode='teacher_forcing', device='cuda:0'):\n",
    "        \"\"\"\n",
    "        Parallel computation.\n",
    "        inputs:\n",
    "            - so # observation sample. # tensor. size: (N, T_obs_max, 2)\n",
    "            - so_lens # T_obs for all samples in sp. # tensor. size: (N,)\n",
    "            - sp # ground truth prediction sample. # tensor. size: (N, T_pred_max, 2)\n",
    "            - sp_lens # T_pred for all samples in sp. # tensor. size: (N,)\n",
    "            - si # intention sample. # tensor. size: (N, 1, 2)\n",
    "            - mode # 'teacher_forcing' or 'no_forcing'.\n",
    "\n",
    "        outputs:\n",
    "            - loss # average offset error across samples.\n",
    "            - sp_pred # prediction on sp. # tensor. size: (N, T_pred_max, 2)\n",
    "        \"\"\"\n",
    "        so_intent = torch.cat((so, si * torch.ones_like(so)), dim=2).float().to(device) # N, T_obs_max, 4\n",
    "        sp_intent = torch.cat((sp, si * torch.ones_like(sp)), dim=2).float().to(device) # N, T_pred_max, 4\n",
    "        si = si.float().to(device)\n",
    "        sp = sp.float().to(device)\n",
    "        sp_mask = padding_mask(sp_lens).float().to(device) # (N, T_pred_max, 1)\n",
    "\n",
    "        batch_size, _, input_channel = so_intent.shape\n",
    "        T_pred_max = sp.shape[1]\n",
    "\n",
    "        so_intent_ebd = self.spatial_embedding(so_intent.reshape(-1, input_channel))\n",
    "        so_intent_ebd = so_intent_ebd.reshape(batch_size, -1, self.embedding_size) # (N, T_obs_max, embed_size)\n",
    "\n",
    "        packed_so_intent_ebd = torch.nn.utils.rnn.pack_padded_sequence(so_intent_ebd, so_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        out, hc_t = self.lstm(packed_so_intent_ebd)\n",
    "\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "\n",
    "        so_last_idx = (torch.ones(batch_size, 1, self.hidden_size)*(so_lens-1).reshape(-1,1,1)).long().to(device) # so_lens-1 -> len-1 is idx\n",
    "        out = out.gather(1, so_last_idx) # dim=1 -> time dimension # (batch_size, 1, hidden_size)\n",
    "\n",
    "        pred_list = []\n",
    "\n",
    "        pred_ts = self.hidden2pos(out.reshape(batch_size, -1)).reshape(batch_size, 1, 2) # (batch, 1, 2)\n",
    "\n",
    "        pred_list.append(pred_ts)\n",
    "        pred_ts_past = pred_ts\n",
    "        for ts in range(1, T_pred_max):\n",
    "            if mode == 'teacher_forcing':\n",
    "                pred_ts_past_intent = sp_intent[:, ts-1:ts] # torch.cat((pred_ts_past, si), dim=2) # (batch, 1, 4) \n",
    "            elif mode == 'no_forcing':\n",
    "                pred_ts_past_intent = torch.cat((pred_ts_past, si), dim=2) # (batch, 1, 4)\n",
    "            pred_ts_past_intent_ebd = self.spatial_embedding(pred_ts_past_intent.reshape(-1, 4))\n",
    "            pred_ts_past_intent_ebd = pred_ts_past_intent_ebd.reshape(batch_size, 1, self.embedding_size)\n",
    "            out, hc_t = self.lstm(pred_ts_past_intent_ebd, hc_t) #(N, 1, embed_size)\n",
    "            pred_ts = self.hidden2pos(out.reshape(batch_size, -1)).reshape(batch_size, 1, 2)\n",
    "            pred_list.append(pred_ts)\n",
    "            pred_ts_past = pred_ts\n",
    "\n",
    "        sp_pred = torch.cat(pred_list, dim=1) # (N, T_pred_max, 2)\n",
    "        loss = average_offset_error(sp, sp_pred, sp_mask)\n",
    "        return loss, sp_pred\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward_origin(self, so, sp, si, mode='teacher_forcing', device='cuda:0'):\n",
    "\n",
    "        \"\"\"\n",
    "        Let's first pretend there is no corner case and everything is fine.\n",
    "        inputs:\n",
    "            - so # observation sample. # tensor. size: (N, obs_seq_len, 2) # already .to(device)\n",
    "            - sp # ground truth prediction sample. # tensor. size: (N, pred_seq_len, 2) # already .to(device)\n",
    "            - si # intention sample. # tensor. size: (N, 1, 2) # already .to(device)\n",
    "            - mode # 'teacher_forcing' or 'no_forcing'.\n",
    "\n",
    "        outputs:\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_size = so.size(0) # batch_size = \n",
    "        sio = torch.ones_like(so).to(device) * si # attach to so\n",
    "        so_ebd = torch.cat((so, sio), dim=2) # (1, obs_seq_len, 4)\n",
    "        so_ebd = self.spatial_embedding(so_ebd.reshape(-1, 4))\n",
    "\n",
    "        so_ebd = so_ebd.reshape(batch_size, -1, self.embedding_size)\n",
    "        hc_0 = self.init_hidden(batch_size, device=device)\n",
    "        out, hc_t = self.lstm(so_ebd, hc_0) #  out: (batch_size, obs_seq_len, hidden_size)\n",
    "        hf = out[:, -1:, :] # (batch_size, 1, hidden_size)\n",
    "\n",
    "        sp_pred = torch.zeros_like(sp)\n",
    "        sp_pred_ts = self.hidden2pos(hf.reshape(batch_size, -1)).reshape(batch_size, 1, 2)\n",
    "        sp_pred[:, 0:1, :] = sp_pred_ts\n",
    "        sp_pred_ts_prev = sp_pred_ts\n",
    "\n",
    "        pred_seq_len = sp.size(1)\n",
    "\n",
    "        for ts in range(1, pred_seq_len):\n",
    "            if mode == 'teacher_forcing':\n",
    "                sp_ts_prev = sp[:, ts-1:ts] # (batch_size, 1, 2)\n",
    "                sp_pred_ts_prev_ebd = torch.cat((sp_ts_prev, si), dim=2) # (1, 1, 4)\n",
    "                sp_pred_ts_prev_ebd = self.spatial_embedding(sp_pred_ts_prev_ebd .reshape(-1, 4)) # embedding from ground truth data\n",
    "            elif mode == 'no_forcing':\n",
    "                sp_pred_ts_prev_ebd = torch.cat((sp_pred_ts_prev, si), dim=2) # (1, 1, 4)\n",
    "                sp_pred_ts_prev_ebd = self.spatial_embedding(sp_pred_ts_prev_ebd.reshape(-1, 4))\n",
    "            else:\n",
    "                print('Error on mode.')\n",
    "                sys.exit(1)\n",
    "\n",
    "            sp_pred_ts_prev_ebd = sp_pred_ts_prev_ebd.reshape(batch_size, 1, self.embedding_size)\n",
    "\n",
    "            out, hc_t = self.lstm(sp_pred_ts_prev_ebd, hc_t)\n",
    "            hf = out[:, -1:, :]\n",
    "            \n",
    "            sp_pred_ts = self.hidden2pos(hf.reshape(batch_size, -1)).reshape(batch_size, 1, 2)\n",
    "            sp_pred[:, ts:ts+1, :] = sp_pred_ts\n",
    "            sp_pred_ts_prev = sp_pred_ts\n",
    "\n",
    "#         loss = torch.mean(((sp_pred-sp) ** 2).sum(2))\n",
    "        loss = torch.mean((((sp_pred-sp) ** 2).sum(2))**(1./2))\n",
    "\n",
    "        return loss, sp_pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed7b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isdir, join\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import rosbag\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import os\n",
    "import pdb\n",
    "import math\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "from os.path import join\n",
    "from os import makedirs\n",
    "import time\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8046e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = 'short_prediction_data.pt'\n",
    "dataset = torch.load(dataset_filepath)\n",
    "\n",
    "xobs_train, xpred_train, yintention_train, xobs_test, xpred_test, yintention_test = \\\n",
    "dataset[\"xobs_train\"], dataset[\"xpred_train\"], dataset[\"yintention_train\"], dataset[\"xobs_test\"], \\\n",
    "dataset[\"xpred_test\"], dataset[\"yintention_test\"]\n",
    "obs_seq_len, pred_seq_len = dataset[\"obs_seq_len\"], dataset[\"pred_seq_len\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0551e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=32,\n",
    "        obs_seq_len=4,\n",
    "        pred_seq_len=6,\n",
    "        embedding_size=32,\n",
    "        hidden_size=32,\n",
    "        num_layers=1,\n",
    "        dropout=0.,\n",
    "        lr=1e-3,\n",
    "        num_epochs=100,\n",
    "        clip_grad=10.,\n",
    "        device=\"cuda:0\",\n",
    "        checkpoint_dir=\"checkpoints\",\n",
    "        ):\n",
    "        self.batch_size = batch_size\n",
    "        self.obs_seq_len = obs_seq_len\n",
    "        self.pred_seq_len = pred_seq_len\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.clip_grad = clip_grad\n",
    "        self.device = device\n",
    "        self.checkpoint_dir=checkpoint_dir\n",
    "\n",
    "args = Arguments(\n",
    "    obs_seq_len=obs_seq_len,\n",
    "    pred_seq_len=pred_seq_len,\n",
    "    lr=5e-3,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5395b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoriesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        xobs,\n",
    "        xpred,\n",
    "        yintention,\n",
    "        obs_seq_len=4,\n",
    "        pred_seq_len=6,\n",
    "        ):\n",
    "        super(TrajectoriesDataset, self).__init__()\n",
    "        assert xobs.shape[0]==xpred.shape[0]==yintention.shape[0]\n",
    "        assert xobs.shape[1]==obs_seq_len and xpred.shape[1]==pred_seq_len\n",
    "        self.obs_seq_len = obs_seq_len\n",
    "        self.pred_seq_len = pred_seq_len\n",
    "        self.seq_len = self.obs_seq_len + self.pred_seq_len\n",
    "        self.xobs = xobs\n",
    "        self.xpred = xpred\n",
    "        self.yintention = yintention\n",
    "        self.num_seq = self.xobs.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_seq\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        out = [\n",
    "            self.xobs[index],\n",
    "            self.xpred[index],\n",
    "            self.yintention[index],\n",
    "        ]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fad41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "for train_index, validation_index in kf.split(xobs_train):\n",
    "    xobs_train_k, xobs_val_k = xobs_train[train_index], xobs_train[validation_index]\n",
    "    xpred_train_k, xpred_val_k = xpred_train[train_index], xpred_train[validation_index] \n",
    "    yintention_train_k, yintention_val_k = yintention_train[train_index], yintention_train[validation_index]\n",
    "    dataset_train = TrajectoriesDataset(\n",
    "        xobs_train_k,\n",
    "        xpred_train_k,\n",
    "        yintention_train_k,\n",
    "        obs_seq_len=obs_seq_len,\n",
    "        pred_seq_len=pred_seq_len,\n",
    "    )\n",
    "    dataset_val = TrajectoriesDataset(\n",
    "        xobs_val_k,\n",
    "        xpred_val_k,\n",
    "        yintention_val_k,\n",
    "        obs_seq_len=obs_seq_len,\n",
    "        pred_seq_len=pred_seq_len,\n",
    "    )\n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,  \n",
    "    )\n",
    "    loader_val = DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,  \n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b963e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentionLstm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size=32,\n",
    "        hidden_size=32,\n",
    "        num_layers=1,\n",
    "        dropout=0.,\n",
    "        bidirectional=False,\n",
    "        obs_seq_len=4,\n",
    "        pred_seq_len=6,\n",
    "    ):\n",
    "        super(IntentionLstm, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=2*embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            )\n",
    "        self.spatial_embedding = nn.Linear(2, embedding_size)\n",
    "        self.intention_embedding = nn.Linear(1, embedding_size)\n",
    "        if bidirectional:\n",
    "            self.directions = 2\n",
    "        else:\n",
    "            self.directions = 1\n",
    "        self.hidden_to_pos = nn.Linear(self.directions*num_layers*hidden_size, 2)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.obs_seq_len = obs_seq_len\n",
    "        self.pred_seq_len = pred_seq_len\n",
    "    \n",
    "    def forward(self, b_xobs, b_yintention, device=\"cuda:0\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward function.\n",
    "        inputs:\n",
    "            - b_xobs: batch of observation. (batch, obs_seq_len, 2)\n",
    "            - b_yintention: batch of intention. (batch, 1)\n",
    "        outputs:\n",
    "            - b_xpred: (batch, pred_seq_len, 2)\n",
    "            \n",
    "        \"\"\"\n",
    "        batch_size, _, _ = b_xobs.shape\n",
    "        b_xobs = self.spatial_embedding(b_xobs) # (batch, obs_seq_len, embedding_size)\n",
    "        b_yintention = self.intention_embedding(b_yintention.unsqueeze(-1)) # (batch, 1, embedding_size)\n",
    "        b_yintention_obs = b_yintention*torch.ones(batch_size, self.obs_seq_len, self.embedding_size).to(device)\n",
    "        b_obs = torch.cat((b_xobs, b_yintention_obs),dim=2) # (batch, obs_seq_len, 2*embedding_size)\n",
    "        _, (ht, ct) = self.lstm(b_obs) # (Dâˆ—num_layers, batch, hidden_size)\n",
    "        b_xpred = []\n",
    "        b_xpred_tt = self.hidden_to_pos(ht.permute(1,0,2).reshape(batch_size,1,-1)) # (batch, 1, 2)\n",
    "        b_xpred.append(b_xpred_tt)\n",
    "        for tt in range(1, self.pred_seq_len):\n",
    "            b_pred_tt = torch.cat((self.spatial_embedding(b_xpred_tt), b_yintention), dim=2)\n",
    "            _, (ht, ct) = self.lstm(b_pred_tt, (ht, ct))\n",
    "            b_xpred_tt = self.hidden_to_pos(ht.permute(1,0,2).reshape(batch_size,1,-1)) # (batch, 1, 2)\n",
    "            b_xpred.append(b_xpred_tt)\n",
    "        b_xpred = torch.cat(b_xpred, dim=1)\n",
    "        return b_xpred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b63181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, loader_train, loader_val):\n",
    "    model = IntentionLstm(\n",
    "        embedding_size=args.embedding_size,\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_layers=args.num_layers,\n",
    "        dropout=args.dropout,\n",
    "        bidirectional=False,\n",
    "        obs_seq_len=args.obs_seq_len,\n",
    "        pred_seq_len=args.pred_seq_len,\n",
    "    ).to(args.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    now = datetime.now()\n",
    "    curr_checkpoint_dir = join(args.checkpoint_dir, now.strftime(\"%y%m%d_%H%M%S\"))\n",
    "    if not isdir(curr_checkpoint_dir):\n",
    "        makedirs(curr_checkpoint_dir)\n",
    "    with open(join(curr_checkpoint_dir, 'args.pickle'), 'wb') as f:\n",
    "        pickle.dump(args, f)\n",
    "    print('EPOCHS: ', args.num_epochs)\n",
    "    train_loss_task, train_aoe_task, train_foe_task = [], [], []\n",
    "    val_loss_task, val_aoe_task, val_foe_task = [], [], []\n",
    "    \n",
    "    for epoch in range(1, args.num_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss_epoch, train_aoe_epoch, train_foe_epoch = [], [], []\n",
    "        for batch_idx, batch in enumerate(loader_train):\n",
    "            batch = [b.to(args.device) for b in batch]\n",
    "            b_xobs, b_xpred_gt, b_yintention = batch\n",
    "            optimizer.zero_grad()\n",
    "            b_xpred = model(b_xobs, b_yintention, device=args.device) # (batch, pred_seq_len, 2)\n",
    "            loss = ((b_xpred-b_xpred_gt)**2.).mean()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n",
    "            optimizer.step()\n",
    "            offset_error = (((b_xpred-b_xpred_gt)**2.).sum(2))**0.5 # (batch, pred_seq_len)\n",
    "            aoe, foe = offset_error.mean(-1).mean(), offset_error[-1].mean()\n",
    "            train_loss_epoch.append(loss.detach().to('cpu').item())\n",
    "            train_aoe_epoch.append(aoe.detach().to('cpu').item())\n",
    "            train_foe_epoch.append(foe.detach().to('cpu').item())\n",
    "        train_loss_epoch = sum(train_loss_epoch)/len(train_loss_epoch)\n",
    "        train_aoe_epoch = sum(train_aoe_epoch)/len(train_aoe_epoch)\n",
    "        train_foe_epoch = sum(train_foe_epoch)/len(train_foe_epoch)\n",
    "        val_loss_epoch, val_aoe_epoch, val_foe_epoch = inference(args, loader_val, model)\n",
    "        if epoch % 10 == 0:\n",
    "#             print('Epoch: {0} | train loss: {1:.2f} | val loss: {2:.2f} | train aoe: {3:.2f} | val aoe: {4:.2f} | train foe: {5:.2f} | val foe: {6:.2f} | period: {7:.2f} sec'\\\n",
    "#                 .format(epoch, train_loss_epoch, val_loss_epoch,\\\n",
    "#                 train_aoe_epoch, val_aoe_epoch,\\\n",
    "#                 train_foe_epoch, val_foe_epoch,\\\n",
    "#                 time.time()-epoch_start_time)) \n",
    "            print('Epoch: {0} | train aoe: {1:.2f} | val aoe: {2:.2f} | train foe: {3:.2f} | val foe: {4:.2f} | period: {5:.2f} sec'\\\n",
    "                .format(epoch, \\\n",
    "                train_aoe_epoch, val_aoe_epoch,\\\n",
    "                train_foe_epoch, val_foe_epoch,\\\n",
    "                time.time()-epoch_start_time)) \n",
    "            model_filename = join(curr_checkpoint_dir, 'epoch_'+str(epoch)+'.pt')\n",
    "            torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': train_loss_epoch,\n",
    "                    'val_loss': val_loss_epoch,\n",
    "                    'train_aoe': train_aoe_epoch,\n",
    "                    'val_aoe': val_aoe_epoch, \n",
    "                    'train_foe': train_foe_epoch,\n",
    "                    'val_foe': val_foe_epoch,   \n",
    "                    }, model_filename)\n",
    "            # print('epoch_'+str(epoch)+'.pt is saved.')\n",
    "        train_loss_task.append(train_loss_epoch)\n",
    "        train_aoe_task.append(train_aoe_epoch)\n",
    "        train_foe_task.append(train_foe_epoch)\n",
    "        val_loss_task.append(val_loss_epoch)\n",
    "        val_aoe_task.append(val_aoe_epoch)\n",
    "        val_foe_task.append(val_foe_epoch)\n",
    "    hist = {}\n",
    "    hist['train_loss'], hist['val_loss'] = train_loss_task, val_loss_task\n",
    "    hist['train_aoe'], hist['val_aoe'] = train_aoe_task, val_aoe_task\n",
    "    hist['train_foe'], hist['val_foe'] = train_foe_task, val_foe_task\n",
    "    with open(join(curr_checkpoint_dir, 'train_hist.pickle'), 'wb') as f:\n",
    "        pickle.dump(hist, f)\n",
    "        print(join(curr_checkpoint_dir, 'train_hist.pickle')+' is saved.')\n",
    "    return\n",
    "\n",
    "\n",
    "def inference(args, loader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_epoch, aoe_epoch, foe_epoch = [], [], []\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            batch = [b.to(args.device) for b in batch]\n",
    "            b_xobs, b_xpred_gt, b_yintention = batch\n",
    "            b_xpred = model(b_xobs, b_yintention, device=args.device) # (batch, pred_seq_len, 2)\n",
    "            loss = ((b_xpred-b_xpred_gt)**2.).mean()\n",
    "            offset_error = (((b_xpred-b_xpred_gt)**2.).sum(2))**0.5 # (batch, pred_seq_len)\n",
    "            aoe, foe = offset_error.mean(-1).mean(), offset_error[-1].mean()\n",
    "            loss_epoch.append(loss.detach().to('cpu').item())\n",
    "            aoe_epoch.append(aoe.detach().to('cpu').item())\n",
    "            foe_epoch.append(foe.detach().to('cpu').item())\n",
    "        loss_epoch = sum(loss_epoch)/len(loss_epoch)\n",
    "        aoe_epoch = sum(aoe_epoch)/len(aoe_epoch)\n",
    "        foe_epoch = sum(foe_epoch)/len(foe_epoch)\n",
    "    return loss_epoch, aoe_epoch, foe_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e49074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS:  100\n",
      "Epoch: 10 | train aoe: 297.71 | val aoe: 294.10 | train foe: 305.80 | val foe: 300.37 | period: 0.32 sec\n",
      "Epoch: 20 | train aoe: 252.74 | val aoe: 249.53 | train foe: 254.81 | val foe: 256.25 | period: 0.32 sec\n",
      "Epoch: 30 | train aoe: 209.17 | val aoe: 206.42 | train foe: 214.65 | val foe: 213.68 | period: 0.32 sec\n",
      "Epoch: 40 | train aoe: 167.14 | val aoe: 165.08 | train foe: 168.63 | val foe: 173.00 | period: 0.32 sec\n",
      "Epoch: 50 | train aoe: 127.65 | val aoe: 126.36 | train foe: 120.79 | val foe: 135.07 | period: 0.32 sec\n",
      "Epoch: 60 | train aoe: 92.19 | val aoe: 92.32 | train foe: 88.53 | val foe: 102.04 | period: 0.31 sec\n",
      "Epoch: 70 | train aoe: 66.50 | val aoe: 68.79 | train foe: 61.66 | val foe: 79.27 | period: 0.31 sec\n",
      "Epoch: 80 | train aoe: 58.05 | val aoe: 60.65 | train foe: 66.23 | val foe: 64.72 | period: 0.31 sec\n",
      "Epoch: 90 | train aoe: 58.01 | val aoe: 60.39 | train foe: 70.35 | val foe: 61.76 | period: 0.32 sec\n",
      "Epoch: 100 | train aoe: 58.03 | val aoe: 60.40 | train foe: 55.00 | val foe: 61.82 | period: 0.31 sec\n",
      "checkpoints/211015_213739/train_hist.pickle is saved.\n"
     ]
    }
   ],
   "source": [
    "train(args, loader_train, loader_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
